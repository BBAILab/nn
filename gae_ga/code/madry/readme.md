# Adversarial Example generation with Madry et al. PGD Method &  Validation of Adversarial Examples

This folder contains programs and files for computing the percentage of adversarial examples generated by 
the Madry et al. method are properly classified by the originating neural network.  That is the percentage 
of adversarial examples that are not classified differently than their associated MNIST image.

All of the code is based on the code in the MadryLab GitHub repositoty, 
[GitHub_MadryLab](https://github.com/MadryLab/mnist_challenge), which we have revised as needed.

GitHub file size restrictions prohibit uploading the files of adversarial examples that we cited in the
accompanying paper and, the neural network training checkpoint files.  To reproduce our computations regarding 
the validation percentage, a new neural network must be trained and, subsequently, used to create adversarial 
examples for the MNIST training set.  A work flow to accomplish two tasks, 
(i) generating a file of adversarial examples using the Madry PGD method for the Madry neural network and 
(ii) computing the percentage of adversarial examples recognized by the originating Madry neural network,
are as is shown below. 

1. Train model with natural MNIST images
    - Execute train_nat_jrb.py
	- This program trains the neural network and creates checkpoint files in the ./models/natural/home folder that will subsequently be read to instantiate the trained neural network for generating adversarial examples.
2. Create adversarial examples
    - execute pgd_attack_train_jrb.py
	- This program creates multiple files with data about the adversarial examples:
	    - ./attacks/nat_trained/attacked_train.npy: a numpy text file uploaded later to check percentage of properly classified adversrial examples
		- ./attacks/nat_trained/madry_adv_eg_nat_trained.csv: adversarial example data we used in our research where the first field is the ground truth of the MNIST image and the remaining 784 values are the adversarial example image data.
3. Compute percentage of adversarial examples recognized by the originating Madry et al. neural network
    - Execute madry_validity_check_jrb.py
      - A program that reads the adversarial examples from ./attacks/nat_trained/attack_train.npy, classifies them with the original Madry et al. neural network used to create them, and computes the percentage of properly classified images.
	- execute run_attack_jrb.py
	  - Code from Madry revised to evaluate the classification of the 60000 adversarial examples based on the MNIST training set, similarly to the code in the bullet above.
