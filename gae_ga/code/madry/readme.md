# Adversarial Example Generation with Madry et al. PGD Method &  Validation of Adversarial Examples

This folder contains programs and files for computing the percentage of adversarial examples generated by 
the Madry et al. method are properly classified by the originating neural network.  That is the percentage 
of adversarial examples that are not classified differently than their associated MNIST image.

All of the code is based on the code in the MadryLab GitHub repositoty, 
[GitHub_MadryLab](https://github.com/MadryLab/mnist_challenge), which we have revised as needed.

This code requires an Anaconda/Python environment with these packages:
  - keras
  - tensorflow-gpu (version 1.14)

GitHub file size restrictions prohibit uploading the files of adversarial examples that we cited in the
accompanying paper and, the neural network training checkpoint files.  To reproduce our computations regarding 
the validation percentage, a new neural network must be trained and, subsequently, used to create adversarial 
examples for the MNIST training set.  A work flow to accomplish two tasks, 
(i) generating a file of adversarial examples using the Madry PGD method for the Madry neural network and 
(ii) computing the percentage of adversarial examples recognized by the originating Madry neural network,
are as is shown below. Note that traineing a neural network will result is a different set of weights than those 
we used, and so the percentage of correctly classified adversarial examples will be different for your model than 
it was for ours.

<!--- Caveat: It is a known problem with Tesnorflow 1.14 (possibly likely depending on your installation) that you will need to restart the Python interpreter from one step to another when they use Tensorflow to avoid an error in loading the model.  --->

<!--- 1. Download Madry checkpoint files
  - From the command line execute this statement: python fetch_model.py natural
  - This original Madry program will create a folder named models and place some checkpoint files within it
2. Create a file of adversarial examples that will be used during training
  - Execute the original Madry files pgd_attack.py
  - This creates a numpy file in the attacks/ folder with the name specified in the config.json file
3. Delete all files in the models/ subfolder
  - Training the neural network anew will create new checkpoint files that we will use.  --->
1. Train model with natural MNIST images
    - Execute train_nat_jrb.py
	- This program trains the neural network and creates checkpoint files in the ./models/natural/home folder that will subsequently be read to instantiate the trained neural network for generating adversarial examples.
2. Create adversarial examples
    - Execute pgd_attack_train_jrb.py
	- This program creates a file with adversarial examples:
	    - ./attacks/nat_trained/attack_train.npy: a numpy text file uploaded later to check percentage of properly classified adversrial examples
		<!--- ./attacks/nat_trained/madry_adv_eg_nat_trained.csv: adversarial example data we used in our research where the first field is the ground truth of the MNIST image and the remaining 784 values are the adversarial example image data. --->
3. Compute percentage of adversarial examples recognized by the originating Madry et al. neural network
    - Execute madry_validity_check_jrb.py
      - A program that reads the adversarial examples from ./attacks/nat_trained/attack_train.npy, classifies them with the original Madry et al. neural network used to create them, and computes the percentage of properly classified images.
<!--- 	- Execute run_attack_jrb.py
	  - Code from Madry revised to evaluate the classification of the 60000 adversarial examples based on the MNIST training set, similarly to the code in the bullet above. --->

# Computational Experience

We found restoring checkpoint files to be unreliable, requiring an uncertain sequence of statements to restore the file.  In some cases, restoring 
the model twice was necessary to instantiate the saved model weights rather than the random initial values.  Other times, restoring once was sufficient and 
restoring twice caused the model to be loaded twice, which threw an error.  Even on the same machine with the same code with the same starting state in terms of
IDE state, no certain factors were discovered to determine a priori which sequence of statements was necessary.  

Restoring a checkpoint is required in pd_attack_train_jrb.py and madry_validity_check_jrb.py.  In all cases the following statements were executed to 
clear any existing tensorflow graphs, because loading another graph adds to the existing graph rather than replacing it, and instantiating a saver.

```python
tf.compat.v1.reset_default_graph()
model = Model()
saver = tf.train.Saver()
```

Then, within the session block, these statements were executed if the model needed to be loaded twice.  IF not, then the last three statements were ommitted or 
commented out.

```python
saver.restore(sess, model_file)
model = Model()
saver = tf.compat.v1.train.Saver()
saver.restore(sess, model_file)
```