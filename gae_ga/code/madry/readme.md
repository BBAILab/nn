# Adversarial Example Generation with Madry et al. PGD Method &  Validation of Adversarial Examples

This folder contains programs and files for computing the percentage of adversarial examples generated by 
the Madry et al. method are properly classified by the originating neural network.  That is the percentage 
of adversarial examples that are not classified differently than their associated MNIST image.

All of the code is based on the code in the MadryLab GitHub repositoty, 
[GitHub_MadryLab](https://github.com/MadryLab/mnist_challenge), which we have been revised as needed.

This code requires an Anaconda/Python environment with these packages:
  - keras
  - tensorflow-gpu (version 1.14)

GitHub file size restrictions prohibit uploading the adversarial examples that gave rise to the observation that 
99% of adversarial examples generated by the method in Madry et al. were properly classified by the originating neural network.  
We provide in this repo, therfore, the code we used to train the network, generate the adversarial examples, and 
predict their classification label using the originating neural network.    The workflow we followed is noted below.  Note 
that training a new neural network will result is a different set of weights than those 
we used, and so the percentage of correctly classified adversarial examples will be different for your model than 
it was for ours.  A zip file of a the code in this repository with the adversarial examples is available at https://wm1693.box.com/v/bbailab-madry-code . 

Before executing the steps below start with an IDE with a fresh Python interpreter to ensure 
that no tensorflow graph exists already.  Otherwise, the new model will be added to that 
graph rather than replacing it.

<!--- Caveat: It is a known problem with Tesnorflow 1.14 (possibly likely depending on your installation) that you will need to restart the Python interpreter from one step to another when they use Tensorflow to avoid an error in loading the model.  --->

<!--- 1. Download Madry checkpoint files
  - From the command line execute this statement: python fetch_model.py natural
  - This original Madry program will create a folder named models and place some checkpoint files within it
2. Create a file of adversarial examples that will be used during training
  - Execute the original Madry files pgd_attack.py
  - This creates a numpy file in the attacks/ folder with the name specified in the config.json file
3. Delete all files in the models/ subfolder
  - Training the neural network anew will create new checkpoint files that we will use.  --->
1. Start with a fresh IDE session or, in the case of Spyder, restart the kernel.
2. Train model with natural MNIST images
    - Execute train_nat_jrb.py
	- This program trains the neural network and creates checkpoint files in the ./models/natural/home folder that will subsequently be read to instantiate the trained neural network for generating adversarial examples.
3. Create adversarial examples
    - Execute pgd_attack_train_jrb.py
	- This program creates a file with adversarial examples:
	    - ./attacks/nat_trained/attack_train.npy: a numpy text file uploaded later to check percentage of properly classified adversrial examples
		<!--- ./attacks/nat_trained/madry_adv_eg_nat_trained.csv: adversarial example data we used in our research where the first field is the ground truth of the MNIST image and the remaining 784 values are the adversarial example image data. --->
4. Compute percentage of adversarial examples recognized by the originating Madry et al. neural network
    - Execute madry_validity_check_jrb.py
      - A program that reads the adversarial examples from ./attacks/nat_trained/attack_train.npy, classifies them with the original Madry et al. neural network used to create them, and computes the percentage of properly classified images.
<!--- 	- Execute run_attack_jrb.py
	  - Code from Madry revised to evaluate the classification of the 60000 adversarial examples based on the MNIST training set, similarly to the code in the bullet above. --->

# Computational Experience

We found restoring checkpoint files to be unreliable, requiring an uncertain sequence of statements to restore the file.  In some cases, restoring 
the model twice was necessary to instantiate the saved model weights rather than the random initial values.  Other times, restoring once was sufficient and 
restoring twice caused the model to be loaded twice, which threw an error.  Even on the same machine with the same code with the same starting state in terms of
IDE state, no certain factors were discovered to determine a priori which sequence of statements was necessary.  

Restoring a checkpoint is required in pd_attack_train_jrb.py and madry_validity_check_jrb.py.  In all cases the following statements were executed to 
clear any existing tensorflow graphs, because loading another graph adds to the existing graph rather than replacing it, and instantiating a saver.

```python
tf.compat.v1.reset_default_graph()
model = Model()
saver = tf.train.Saver()
```

Then, within the session block, these statements were executed if the model needed to be loaded twice.  If not, then the last three statements were ommitted or 
commented out.

```python
saver.restore(sess, model_file)
model = Model()
saver = tf.compat.v1.train.Saver()
saver.restore(sess, model_file)
```

We found idiosyncracies in execution even when the steps above were taken.  The kernel (in spyder) needed to occasionally be restarted and/or Spyder needed to be restarted.

# Reference

Madry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu, Towards Deep Learning Models Resistant to Adversarial Attacks, 2017, https://arxiv.org/abs/1706.06083