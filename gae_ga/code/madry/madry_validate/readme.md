# Validation of Madry et al. Adversarial Examples

This folder contains programs and files for computing the percentage of adversarial examples generated by 
the Madry et al. method are properly classified by the originating neural network.  That is the percentage 
of adversarial examples that are not classified differently than their associated MNIST image.

GitHub file size restrictions prohibit uploading the files of adversarial examples generated and the neural 
network training checkpoint files.  To compute the validation percentage, the workflow below must, 
therfore, be executed.

1. Train model with natural MNIST images
    - train_nat_jrb.py
	- This program trains the neural network and created checkpoint files in the ./models/natural/home/ folder the will subsequently be read to instantiate the trained neural network for generating adversarial exampels.
2. Create adversarial examples
    - pgd_attack_train_jrb.py
	- Creates multiple files with data about the adversarial examples:
	    - ./attacks/nat_trained/attacked_train.npy: a numpy text file uploaded later to check percentage of properly classified adversrial examples
		- ./attacks/nat_trained/madry_adv_eg_nat_trained.csv: adversarial example data we used in our research where the first field is the ground truth of the MNIST image and the remaining 784 values are the adversarial example image data.
3. Evaluate percentage of adversarial examples recognized by the network
    - madry_validity_check_jrb.py: a program that reads the adversarial examples from ./attacks/nat_trained/attack_train.npy , classifies them with the original neural network used to create them, and compute the percentage of properly classified images.
	- run_attack_jrb.py: code from Madry revised to evaluate the classification of the 60000 adversarial examples based on the MNIST training set, similarly to the code in the bullet above.
    